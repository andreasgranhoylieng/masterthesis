{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does not work for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syringe_keypoint_training_final.py\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from fvcore.common.config import CfgNode as CN\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog, \n",
    "    DatasetCatalog, \n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    "    DatasetMapper\n",
    ")\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import detectron2.data.transforms as T\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "# --------------------------\n",
    "# 1. Dataset Configuration\n",
    "# --------------------------\n",
    "def filter_and_register_dataset(json_path: str, img_dir: str, dataset_name: str) -> str:\n",
    "    \"\"\"Robust dataset registration with validation.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"Annotation file missing: {json_path}\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        raise NotADirectoryError(f\"Image directory missing: {img_dir}\")\n",
    "\n",
    "    with open(json_path) as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Identify the syringe category\n",
    "    syringe1_id = next(c[\"id\"] for c in coco_data[\"categories\"] if c[\"name\"] == \"syringe1\")\n",
    "    \n",
    "    # Filter annotations\n",
    "    valid_annos = []\n",
    "    for a in coco_data[\"annotations\"]:\n",
    "        if a[\"category_id\"] == syringe1_id:\n",
    "            kps = np.array(a[\"keypoints\"], dtype=np.float32).reshape(-1, 3)\n",
    "            if kps.shape[1] != 3:\n",
    "                raise ValueError(f\"Invalid keypoints in image {a['image_id']}\")\n",
    "            a[\"keypoints\"] = kps.reshape(-1).tolist()\n",
    "            a[\"num_keypoints\"] = kps.shape[0]\n",
    "            valid_annos.append(a)\n",
    "    \n",
    "    valid_img_ids = {a[\"image_id\"] for a in valid_annos}\n",
    "    valid_imgs = [img for img in coco_data[\"images\"] if img[\"id\"] in valid_img_ids]\n",
    "    \n",
    "    # Make sure categories have \"keypoints\" and \"skeleton\"\n",
    "    categories = []\n",
    "    for c in coco_data[\"categories\"]:\n",
    "        if c[\"name\"] == \"syringe1\":\n",
    "            c[\"keypoints\"] = [\n",
    "                \"plunger_top\",\n",
    "                \"plunger_bottom\",\n",
    "                \"syringe_top\",\n",
    "                \"syringe_bottom\"\n",
    "            ]\n",
    "            # Example skeleton (1-based indexing in COCO):\n",
    "            # Connect (plunger_top <-> plunger_bottom) and (syringe_top <-> syringe_bottom)\n",
    "            c[\"skeleton\"] = [[1, 2], [3, 4]]\n",
    "            categories.append(c)\n",
    "    \n",
    "    filtered_data = {\n",
    "        \"images\": valid_imgs,\n",
    "        \"annotations\": valid_annos,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    filtered_json = os.path.join(os.path.dirname(json_path), f\"filtered_{dataset_name}.json\")\n",
    "    with open(filtered_json, 'w') as f:\n",
    "        json.dump(filtered_data, f)\n",
    "    \n",
    "    try:\n",
    "        register_coco_instances(dataset_name, {}, filtered_json, img_dir)\n",
    "        if len(DatasetCatalog.get(dataset_name)) == 0:\n",
    "            raise ValueError(f\"Empty dataset after registration: {dataset_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dataset registration failed: {str(e)}\")\n",
    "        exit(1)\n",
    "    \n",
    "    return filtered_json\n",
    "\n",
    "# Initialize datasets FIRST\n",
    "dataset_base = \"/Users/andreas/Desktop/repos/masterthesis/datasets/Syringe-volume-estimation-detectron2\"\n",
    "try:\n",
    "    train_json = filter_and_register_dataset(\n",
    "        os.path.join(dataset_base, \"train/_annotations.coco.json\"),\n",
    "        os.path.join(dataset_base, \"train\"),\n",
    "        \"syringe_train\"\n",
    "    )\n",
    "    val_json = filter_and_register_dataset(\n",
    "        os.path.join(dataset_base, \"valid/_annotations.coco.json\"),\n",
    "        os.path.join(dataset_base, \"valid\"),\n",
    "        \"syringe_val\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Dataset initialization failed: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Metadata Configuration\n",
    "# --------------------------\n",
    "syringe_metadata = MetadataCatalog.get(\"syringe_train\")\n",
    "syringe_metadata.thing_classes = [\"syringe1\"]\n",
    "syringe_metadata.keypoint_names = [\"plunger_top\", \"plunger_bottom\", \"syringe_top\", \"syringe_bottom\"]\n",
    "syringe_metadata.keypoint_flip_map = []\n",
    "syringe_metadata.keypoint_connection_rules = [\n",
    "    (\"plunger_top\", \"plunger_bottom\", (0, 255, 0)),\n",
    "    (\"syringe_top\", \"syringe_bottom\", (255, 0, 0)),\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# 3. Model Configuration\n",
    "# --------------------------\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Clear default datasets FIRST\n",
    "cfg.DATASETS.TRAIN = ()\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.MODEL.WEIGHTS = \"\"\n",
    "\n",
    "# Set custom datasets\n",
    "cfg.DATASETS.TRAIN = (\"syringe_train\",)\n",
    "cfg.DATASETS.TEST = (\"syringe_val\",)\n",
    "\n",
    "# Verify dataset registration\n",
    "assert len(DatasetCatalog.get(cfg.DATASETS.TRAIN[0])) > 0, \"No training data registered!\"\n",
    "assert len(DatasetCatalog.get(cfg.DATASETS.TEST[0])) > 0, \"No validation data registered!\"\n",
    "\n",
    "# Device setup\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model architecture\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 4\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.5] * 4\n",
    "\n",
    "# Training parameters\n",
    "cfg.SOLVER.BASE_LR = 0.00002\n",
    "cfg.SOLVER.MAX_ITER = 100\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.WARMUP_ITERS = 100\n",
    "cfg.SOLVER.WARMUP_FACTOR = 0.001\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "cfg.SOLVER.WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# Gradient clipping\n",
    "cfg.SOLVER.CLIP_GRADIENTS = CN()\n",
    "cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
    "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"norm\"\n",
    "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
    "cfg.SOLVER.CLIP_GRADIENTS.NORM_TYPE = 2.0\n",
    "\n",
    "# Validation\n",
    "cfg.TEST.EVAL_PERIOD = 10\n",
    "\n",
    "# Input config\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (600,)\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1200\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1200\n",
    "\n",
    "# Output\n",
    "cfg.OUTPUT_DIR = os.path.join(dataset_base, \"output\")\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Training Hooks\n",
    "# --------------------------\n",
    "class TrainingMonitorHook(HookBase):\n",
    "    def __init__(self, max_iter: int):\n",
    "        self.max_iter = max_iter\n",
    "        self.pbar = None\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def before_train(self):\n",
    "        self.pbar = tqdm(\n",
    "            total=self.max_iter,\n",
    "            desc=\"Training Progress\",\n",
    "            unit=\"iter\",\n",
    "            bar_format=\"{l_bar}{bar:30}{r_bar}\",\n",
    "            postfix={\n",
    "                \"loss\": \"N/A\", \n",
    "                \"iter/s\": \"N/A\",\n",
    "                \"eta\": \"N/A\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def after_step(self):\n",
    "        metrics = self.trainer.storage.latest()\n",
    "        elapsed = time.time() - self.start_time\n",
    "        ips = self.trainer.iter / elapsed if elapsed > 0 else 0\n",
    "        eta = (self.max_iter - self.trainer.iter) / ips if ips > 0 else 0\n",
    "        \n",
    "        self.pbar.set_postfix({\n",
    "            \"loss\": f\"{metrics.get('total_loss', (0,))[0]:.3f}\",\n",
    "            \"iter/s\": f\"{ips:.2f}\",\n",
    "            \"eta\": f\"{eta/3600:.1f}h\" if eta > 3600 else f\"{eta/60:.1f}m\"\n",
    "        })\n",
    "        self.pbar.update(1)\n",
    "        \n",
    "        # Memory management\n",
    "        if self.trainer.iter % 5 == 0:\n",
    "            gc.collect()\n",
    "        \n",
    "    def after_train(self):\n",
    "        self.pbar.close()\n",
    "        print(f\"\\nTraining completed in {(time.time()-self.start_time)/3600:.2f} hours\")\n",
    "\n",
    "class ValidationHook(HookBase):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone()\n",
    "        self.evaluator = COCOEvaluator(\n",
    "            \"syringe_val\",\n",
    "            output_dir=self.cfg.OUTPUT_DIR,\n",
    "            use_fast_impl=False,\n",
    "            kpt_oks_sigmas=cfg.TEST.KEYPOINT_OKS_SIGMAS\n",
    "        )\n",
    "        \n",
    "    def after_step(self):\n",
    "        if self.trainer.iter % self.cfg.TEST.EVAL_PERIOD == 0:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                model = self.trainer.model\n",
    "                original_mode = model.training\n",
    "                \n",
    "                with inference_context(model):\n",
    "                    val_loader = build_detection_test_loader(self.cfg, \"syringe_val\")\n",
    "                    results = inference_on_dataset(model, val_loader, self.evaluator)\n",
    "                    \n",
    "                    print(f\"\\nValidation @ Iter {self.trainer.iter}:\")\n",
    "                    print(json.dumps(results[\"keypoints\"], indent=2))\n",
    "                    print(f\"Validation time: {time.time()-start_time:.1f}s\")\n",
    "                \n",
    "                model.train(original_mode)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️ Validation error: {str(e)}\")\n",
    "                model.train(True)\n",
    "\n",
    "# --------------------------\n",
    "# 5. Training Execution\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify dataset registration\n",
    "    print(\"\\nRegistered datasets:\")\n",
    "    print(f\"Train: {len(DatasetCatalog.get('syringe_train'))} samples\")\n",
    "    print(f\"Validation: {len(DatasetCatalog.get('syringe_val'))} samples\")\n",
    "    \n",
    "    # Verify config values\n",
    "    assert len(cfg.DATASETS.TRAIN) > 0, \"No training datasets configured!\"\n",
    "    assert len(cfg.DATASETS.TEST) > 0, \"No validation datasets configured!\"\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = DefaultTrainer(cfg)\n",
    "    trainer.register_hooks([\n",
    "        TrainingMonitorHook(cfg.SOLVER.MAX_ITER),\n",
    "        ValidationHook(cfg)\n",
    "    ])\n",
    "    \n",
    "    # Model verification\n",
    "    DetectionCheckpointer(trainer.model).resume_or_load(cfg.MODEL.WEIGHTS)\n",
    "    print(\"\\nModel architecture verified:\")\n",
    "    print(trainer.model)\n",
    "    \n",
    "    # Hardware info\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Training on {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "    else:\n",
    "        print(\"⚠️ WARNING: Training on CPU - expect slow performance\")\n",
    "        print(f\"Estimated time: {cfg.SOLVER.MAX_ITER*5/60:.1f} minutes (5s/iter)\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user!\")\n",
    "    finally:\n",
    "        # Final evaluation BEFORE cleanup\n",
    "        final_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            try:\n",
    "                cfg.MODEL.WEIGHTS = final_model_path\n",
    "                evaluator = COCOEvaluator(\"syringe_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "                val_loader = build_detection_test_loader(cfg, \"syringe_val\")\n",
    "                print(\"\\nFinal evaluation results:\")\n",
    "                print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️ Final evaluation failed: {str(e)}\")\n",
    "        \n",
    "        # Cleanup temporary files AFTER evaluation\n",
    "        try:\n",
    "            if os.path.exists(train_json):\n",
    "                os.remove(train_json)\n",
    "            if os.path.exists(val_json):\n",
    "                os.remove(val_json)\n",
    "            print(\"Cleaned temporary files\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Cleanup error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosjektoppgave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
