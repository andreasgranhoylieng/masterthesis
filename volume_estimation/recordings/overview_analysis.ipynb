{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  [markdown]\n",
    "# # YOLO Syringe Volume Estimation Analysis\n",
    "#\n",
    "# This notebook analyzes the performance of a YOLO-based model for estimating liquid volume in syringes.\n",
    "# It includes data loading, preprocessing, calculation of various error metrics, statistical tests, and visualizations.\n",
    "\n",
    "#  [markdown]\n",
    "# ## 1. Setup and Library Imports\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm # This line was causing the error if statsmodels is not installed\n",
    "from statsmodels.formula.api import ols\n",
    "# from statsmodels.stats.multicomp import pairwise_tukeyhsd # Uncomment if needed for post-hoc tests\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"talk\") # \"paper\", \"notebook\", \"talk\", \"poster\"\n",
    "\n",
    "#  [markdown]\n",
    "# ## 2. Data Loading and Initial Inspection\n",
    "#\n",
    "# The data will be loaded from a CSV file.\n",
    "# **You should replace `csv_file_path` with the actual path to your CSV file if it's different from \"overview.csv\".**\n",
    "\n",
    "#\n",
    "# Function to load data\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads data from a CSV file path.\"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    # Correctly handle 'nan' and 'corrupted video' as NaN values across all columns.\n",
    "    df = pd.read_csv(file_path, na_values=['nan', 'corrupted video', 'NaN', 'Corrupted video'])\n",
    "    return df\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "# IMPORTANT: Ensure \"overview.csv\" is in the same directory as the script,\n",
    "# or provide the full path to your CSV file.\n",
    "csv_file_path = \"overview.csv\"\n",
    "df = load_data(file_path=csv_file_path)\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Attempt numeric conversion for key columns immediately after loading\n",
    "print(\"\\nConverting relevant columns to numeric types...\")\n",
    "for col in ['target volume', 'estimated volume', 'container weight']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"Column '{col}' Dtype after to_numeric: {df[col].dtype}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in DataFrame for numeric conversion.\")\n",
    "\n",
    "\n",
    "print(\"\\nDataset information (after initial numeric conversion):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary statistics for numeric columns (initial - after to_numeric):\")\n",
    "# This should now work as expected if conversions were successful for some columns\n",
    "if not df.select_dtypes(include=np.number).empty:\n",
    "    print(df.describe(include=np.number))\n",
    "else:\n",
    "    print(\"No numeric columns found to describe after initial conversion attempt.\")\n",
    "\n",
    "print(\"\\nSummary statistics for object columns (initial):\")\n",
    "if not df.select_dtypes(include='object').empty:\n",
    "    print(df.describe(include='object'))\n",
    "else:\n",
    "    print(\"No object columns found to describe.\")\n",
    "\n",
    "\n",
    "#  [markdown]\n",
    "# ## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "#\n",
    "# Drop rows where essential data for analysis might be missing AFTER initial load\n",
    "# For instance, if 'person', 'target place', 'syringe' is NaN, it's hard to analyze.\n",
    "# 'target volume' NaNs (original or from coerce) will be handled by specific analysis steps.\n",
    "df.dropna(subset=['person', 'target place', 'syringe'], inplace=True)\n",
    "\n",
    "\n",
    "# Extract syringe size (numeric part)\n",
    "# Ensure 'syringe' column exists before trying to extract from it\n",
    "if 'syringe' in df.columns:\n",
    "    df['syringe_size_ml'] = df['syringe'].str.extract(r'(\\d+)').astype(float)\n",
    "else:\n",
    "    print(\"Warning: 'syringe' column not found for 'syringe_size_ml' extraction.\")\n",
    "    df['syringe_size_ml'] = np.nan # Create the column with NaNs if 'syringe' is missing\n",
    "\n",
    "\n",
    "# Define container base weights (empty and with lid)\n",
    "container_specs = {\n",
    "    'throat': {'empty': 4.8, 'lid': 8.0},\n",
    "    'arm': {'empty': 4.9, 'lid': 8.1},\n",
    "    'foot': {'empty': 4.9, 'lid': 8.1}\n",
    "}\n",
    "\n",
    "# Function to determine true container weight based on notes\n",
    "def get_true_container_weight(row: pd.Series) -> float:\n",
    "    \"\"\"Calculates the true weight of the container based on person and target place.\"\"\"\n",
    "    place = row['target place']\n",
    "\n",
    "    if pd.isna(place) or place not in container_specs:\n",
    "        return np.nan\n",
    "\n",
    "    has_lid = False\n",
    "    if pd.notna(row['notes']):\n",
    "        if 'with lid' in str(row['notes']).lower():\n",
    "            has_lid = True\n",
    "        # Special handling for person1 and person2 if notes are ambiguous\n",
    "        elif row['person'] in ['person1', 'person2'] and 'with lid' not in str(row['notes']).lower() and 'no lid' not in str(row['notes']).lower():\n",
    "             has_lid = True\n",
    "\n",
    "\n",
    "    if has_lid:\n",
    "        return container_specs[place]['lid']\n",
    "    else:\n",
    "        return container_specs[place]['empty']\n",
    "\n",
    "df['true_container_weight'] = df.apply(get_true_container_weight, axis=1)\n",
    "\n",
    "# Calculate actual liquid weight (measured) and volume (assuming density 1g/mL)\n",
    "# Ensure 'container weight' and 'true_container_weight' exist and are numeric\n",
    "if 'container weight' in df.columns and 'true_container_weight' in df.columns:\n",
    "    df['actual_liquid_weight_measured'] = df['container weight'] - df['true_container_weight']\n",
    "    df['actual_liquid_volume_from_weight'] = df['actual_liquid_weight_measured'] # Assuming density 1 g/mL\n",
    "else:\n",
    "    print(\"Warning: 'container weight' or 'true_container_weight' missing for liquid weight calculation.\")\n",
    "    df['actual_liquid_weight_measured'] = np.nan\n",
    "    df['actual_liquid_volume_from_weight'] = np.nan\n",
    "\n",
    "\n",
    "# Flag problematic readings based on 'notes'\n",
    "if 'notes' in df.columns:\n",
    "    df['notes_lower'] = df['notes'].str.lower().fillna('')\n",
    "    df['is_occluded'] = df['notes_lower'].str.contains('occlusion|covering syringe')\n",
    "    df['is_bad_reading'] = df['notes_lower'].str.contains('bad reading|horrible reading')\n",
    "    df['is_manual_extraction'] = df['notes_lower'].str.contains('manual extraction')\n",
    "    df['other_issues'] = df['notes_lower'].str.contains('dissaperars|initial condition not met|no syringe detected')\n",
    "    df['problematic_reading'] = df['is_occluded'] | df['is_bad_reading'] | df['is_manual_extraction'] | df['other_issues']\n",
    "else:\n",
    "    print(\"Warning: 'notes' column missing. Cannot create flags for problematic readings.\")\n",
    "    df['is_occluded'] = False\n",
    "    df['is_bad_reading'] = False\n",
    "    df['is_manual_extraction'] = False\n",
    "    df['other_issues'] = False\n",
    "    df['problematic_reading'] = False\n",
    "\n",
    "\n",
    "# Consistency check for target place vs. estimated place\n",
    "if 'target place' in df.columns and 'estimated place' in df.columns:\n",
    "    df['place_match'] = df['target place'] == df['estimated place']\n",
    "    print(\"\\n--- Place Estimation Match (Target vs. Estimated) ---\")\n",
    "    if not df[pd.notna(df['estimated place'])].empty:\n",
    "        print(df[pd.notna(df['estimated place'])]['place_match'].value_counts(normalize=True, dropna=False))\n",
    "    else:\n",
    "        print(\"No non-NaN 'estimated place' data for place match analysis.\")\n",
    "else:\n",
    "    print(\"Warning: 'target place' or 'estimated place' columns missing for place match analysis.\")\n",
    "\n",
    "\n",
    "print(\"\\nDataset after cleaning and preprocessing (info):\")\n",
    "df.info()\n",
    "print(\"\\nSummary statistics for numeric columns (processed):\")\n",
    "numeric_cols_for_summary = ['target volume', 'estimated volume', 'container weight',\n",
    "                            'true_container_weight', 'actual_liquid_volume_from_weight', 'syringe_size_ml']\n",
    "existing_numeric_cols = [col for col in numeric_cols_for_summary if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "if existing_numeric_cols:\n",
    "    print(df[existing_numeric_cols].describe())\n",
    "else:\n",
    "    print(\"No existing numeric columns for final summary statistics.\")\n",
    "\n",
    "\n",
    "# Analysis of rows with NaN in 'estimated volume'\n",
    "if 'estimated volume' in df.columns:\n",
    "    nan_estimated_df = df[df['estimated volume'].isna()]\n",
    "    print(\"\\n--- Analysis of Rows with NaN Estimated Volume (Post-Preprocessing) ---\")\n",
    "    if not nan_estimated_df.empty:\n",
    "        print(f\"Total rows with NaN estimated volume: {len(nan_estimated_df)}\")\n",
    "        if 'target place' in nan_estimated_df: print(\"Counts by 'target place':\\n\", nan_estimated_df['target place'].value_counts(dropna=False))\n",
    "        if 'syringe' in nan_estimated_df: print(\"\\nCounts by 'syringe':\\n\", nan_estimated_df['syringe'].value_counts(dropna=False))\n",
    "        if 'notes_lower' in nan_estimated_df: print(\"\\nCounts from 'notes_lower' (indicative):\\n\", nan_estimated_df['notes_lower'].value_counts(dropna=False).head())\n",
    "\n",
    "        problem_flags_cols = ['is_occluded', 'is_bad_reading', 'is_manual_extraction', 'other_issues']\n",
    "        existing_problem_flags = [col for col in problem_flags_cols if col in nan_estimated_df.columns]\n",
    "        if existing_problem_flags: print(\"\\nProblematic flags for these NaN rows:\\n\", nan_estimated_df[existing_problem_flags].sum())\n",
    "    else:\n",
    "        print(\"No rows with NaN estimated volume found after preprocessing.\")\n",
    "else:\n",
    "    print(\"Warning: 'estimated volume' column not found for NaN analysis.\")\n",
    "\n",
    "\n",
    "#  [markdown]\n",
    "# ## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "#\n",
    "# Histograms of key volumes\n",
    "key_volume_cols = ['target volume', 'estimated volume', 'actual_liquid_volume_from_weight']\n",
    "existing_key_volume_cols = [col for col in key_volume_cols if col in df.columns and df[col].notna().any()]\n",
    "\n",
    "if len(existing_key_volume_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, len(existing_key_volume_cols), figsize=(7 * len(existing_key_volume_cols), 6))\n",
    "    if len(existing_key_volume_cols) == 1: axes = [axes] # Make it iterable\n",
    "\n",
    "    for i, col_name in enumerate(existing_key_volume_cols):\n",
    "        sns.histplot(df[col_name].dropna(), kde=True, ax=axes[i], bins=10).set_title(f'{col_name.replace(\"_\", \" \").title()} Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"volume_distributions_histograms.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No key volume columns with data available for histogram plotting.\")\n",
    "\n",
    "# Scatter plot: Target Volume vs. Actual Liquid Volume from Weight\n",
    "if 'target volume' in df.columns and 'actual_liquid_volume_from_weight' in df.columns and \\\n",
    "   df['target volume'].notna().any() and df['actual_liquid_volume_from_weight'].notna().any():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    hue_col = 'target place' if 'target place' in df.columns else None\n",
    "    style_col = 'person' if 'person' in df.columns else None\n",
    "    sns.scatterplot(x='target volume', y='actual_liquid_volume_from_weight', data=df, hue=hue_col, style=style_col, s=100)\n",
    "\n",
    "    temp_df_for_plot_limits = df[['target volume', 'actual_liquid_volume_from_weight']].dropna()\n",
    "    if not temp_df_for_plot_limits.empty:\n",
    "        max_val_data = temp_df_for_plot_limits.max().max()\n",
    "        min_val_data = temp_df_for_plot_limits.min().min()\n",
    "        plt.plot([min_val_data, max_val_data], [min_val_data, max_val_data],\n",
    "                 'k--', lw=2, label='Identity Line')\n",
    "    plt.title('Target Volume vs. Volume Calculated from Weight')\n",
    "    plt.xlabel('Target Volume (mL)')\n",
    "    plt.ylabel('Actual Liquid Volume from Weight (mL)')\n",
    "    if hue_col or style_col: plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"target_vs_weight_volume_scatter.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping 'Target Volume vs. Volume from Weight' scatter plot due to missing data.\")\n",
    "\n",
    "#  [markdown]\n",
    "# ## 5. YOLO Volume Estimation Performance Analysis\n",
    "#\n",
    "# Here, we analyze the performance of the YOLO model by comparing `estimated volume` against `target volume`.\n",
    "# We will first analyze all valid readings, then consider filtering out problematic ones.\n",
    "\n",
    "#\n",
    "# Ensure 'target volume' and 'estimated volume' exist before proceeding\n",
    "if 'target volume' not in df.columns or 'estimated volume' not in df.columns:\n",
    "    print(\"Critical Error: 'target volume' or 'estimated volume' columns are missing. Cannot proceed with performance analysis.\")\n",
    "    # Optionally exit or skip large sections of the script here\n",
    "    analysis_df = pd.DataFrame() # Empty df to prevent further errors\n",
    "    clean_df = pd.DataFrame()\n",
    "else:\n",
    "    analysis_df = df.dropna(subset=['estimated volume', 'target volume']).copy()\n",
    "\n",
    "    if not analysis_df.empty:\n",
    "        analysis_df['error'] = analysis_df['estimated volume'] - analysis_df['target volume']\n",
    "        analysis_df['abs_error'] = np.abs(analysis_df['error'])\n",
    "        analysis_df['percentage_error'] = np.where(analysis_df['target volume'] != 0, (analysis_df['error'] / analysis_df['target volume']) * 100, np.nan)\n",
    "        analysis_df['abs_percentage_error'] = np.abs(analysis_df['percentage_error'])\n",
    "        if 'syringe_size_ml' in analysis_df.columns:\n",
    "            analysis_df['error_normalized_by_syringe_capacity'] = analysis_df['error'] / analysis_df['syringe_size_ml']\n",
    "            analysis_df['abs_error_normalized_by_syringe_capacity'] = np.abs(analysis_df['error_normalized_by_syringe_capacity'])\n",
    "        else:\n",
    "            analysis_df['error_normalized_by_syringe_capacity'] = np.nan\n",
    "            analysis_df['abs_error_normalized_by_syringe_capacity'] = np.nan\n",
    "\n",
    "        if 'problematic_reading' in analysis_df.columns:\n",
    "            clean_df = analysis_df[~analysis_df['problematic_reading']].copy()\n",
    "        else: # If problematic_reading flag wasn't created (e.g. no 'notes')\n",
    "            print(\"Warning: 'problematic_reading' flag not available. 'clean_df' will be a copy of 'analysis_df'.\")\n",
    "            clean_df = analysis_df.copy()\n",
    "    else:\n",
    "        clean_df = pd.DataFrame() # Empty if analysis_df is empty\n",
    "\n",
    "print(f\"Total valid readings for analysis (non-NaN target & estimated): {len(analysis_df)}\")\n",
    "print(f\"Number of 'clean' readings (no major issues noted): {len(clean_df)}\")\n",
    "if 'problematic_reading' in analysis_df.columns:\n",
    "    print(f\"Number of 'problematic' readings: {len(analysis_df[analysis_df['problematic_reading']])}\")\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true: pd.Series, y_pred: pd.Series, dataset_name: str = \"Overall\") -> dict:\n",
    "    \"\"\"Calculates and prints key performance metrics.\"\"\"\n",
    "    if len(y_true) < 2 or len(y_pred) < 2 or y_true.isnull().all() or y_pred.isnull().all():\n",
    "        print(f\"Not enough valid data (N={len(y_true.dropna())}) for {dataset_name} metrics calculation.\")\n",
    "        return None\n",
    "\n",
    "    y_true_clean = y_true.dropna()\n",
    "    y_pred_clean = y_pred.loc[y_true_clean.index].dropna() # Align and drop NaNs based on y_true\n",
    "    y_true_clean = y_true_clean.loc[y_pred_clean.index] # Ensure same length after y_pred drop\n",
    "\n",
    "    if len(y_true_clean) < 2:\n",
    "        print(f\"Not enough aligned valid data points (N={len(y_true_clean)}) for {dataset_name} metrics.\")\n",
    "        return None\n",
    "\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    mse = mean_squared_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / np.where(y_true_clean == 0, np.finfo(float).eps, y_true_clean))) * 100\n",
    "    mean_error = np.mean(y_pred_clean - y_true_clean)\n",
    "    \n",
    "    try:\n",
    "        cor = np.corrcoef(y_true_clean, y_pred_clean)[0,1]\n",
    "        sd_true = np.std(y_true_clean)\n",
    "        sd_pred = np.std(y_pred_clean)\n",
    "        mean_true_val = np.mean(y_true_clean)\n",
    "        mean_pred_val = np.mean(y_pred_clean)\n",
    "        \n",
    "        if sd_true == 0 or sd_pred == 0 or np.isnan(cor): # Check for NaN correlation\n",
    "            ccc = np.nan\n",
    "        else:\n",
    "            ccc = (2 * cor * sd_true * sd_pred) / (sd_true**2 + sd_pred**2 + (mean_true_val - mean_pred_val)**2)\n",
    "        pearson_corr, _ = stats.pearsonr(y_true_clean, y_pred_clean)\n",
    "\n",
    "    except Exception as e: # Catch any error during correlation calculation (e.g. if data is all constant)\n",
    "        print(f\"Could not calculate correlation for {dataset_name}: {e}\")\n",
    "        cor = np.nan\n",
    "        ccc = np.nan\n",
    "        pearson_corr = np.nan\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"MAPE (%)\": mape,\n",
    "        \"Mean Error (Bias)\": mean_error,\n",
    "        \"Pearson Correlation\": pearson_corr,\n",
    "        \"Lin's CCC\": ccc,\n",
    "        \"Count\": len(y_true_clean)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Performance Metrics for {dataset_name} ---\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            print(f\"{k}: {v:.3f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "    return metrics\n",
    "\n",
    "all_metrics_summary = []\n",
    "if not analysis_df.empty and 'target volume' in analysis_df.columns and 'estimated volume' in analysis_df.columns:\n",
    "    overall_metrics = calculate_metrics(analysis_df['target volume'], analysis_df['estimated volume'], \"All Valid Data\")\n",
    "    if overall_metrics: all_metrics_summary.append(overall_metrics)\n",
    "\n",
    "if not clean_df.empty and 'target volume' in clean_df.columns and 'estimated volume' in clean_df.columns:\n",
    "    clean_metrics = calculate_metrics(clean_df['target volume'], clean_df['estimated volume'], \"Clean Data (No Major Issues)\")\n",
    "    if clean_metrics: all_metrics_summary.append(clean_metrics)\n",
    "\n",
    "\n",
    "# [markdown]\n",
    "# ### 5.1. Visualizations: Scatter Plot and Bland-Altman Plot\n",
    "\n",
    "#\n",
    "def plot_scatter_estimated_vs_target(df_subset: pd.DataFrame, title_suffix: str, filename_suffix: str):\n",
    "    \"\"\"Plots estimated vs. target volume.\"\"\"\n",
    "    if df_subset.empty or len(df_subset.dropna(subset=['target volume', 'estimated volume'])) < 2:\n",
    "        print(f\"No data or insufficient data to plot for {title_suffix}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    hue_col = 'target place' if 'target place' in df_subset.columns else None\n",
    "    style_col = 'syringe_size_ml' if 'syringe_size_ml' in df_subset.columns else None\n",
    "\n",
    "    sns.scatterplot(x='target volume', y='estimated volume', data=df_subset, hue=hue_col, style=style_col, s=120, alpha=0.8)\n",
    "    \n",
    "    temp_df = df_subset[['target volume', 'estimated volume']].dropna()\n",
    "    if not temp_df.empty:\n",
    "        min_val = min(temp_df['target volume'].min(), temp_df['estimated volume'].min())\n",
    "        max_val = max(temp_df['target volume'].max(), temp_df['estimated volume'].max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Identity Line (y=x)')\n",
    "    \n",
    "        if len(temp_df) >= 2:\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(temp_df['target volume'], temp_df['estimated volume'])\n",
    "            plt.plot(temp_df['target volume'], intercept + slope * temp_df['target volume'], 'r-', label=f'Fit: y={slope:.2f}x+{intercept:.2f}\\n$R^2={r_value**2:.2f}$')\n",
    "\n",
    "    plt.title(f'Estimated Volume vs. Target Volume ({title_suffix})', fontsize=16)\n",
    "    plt.xlabel('Target Volume (mL)', fontsize=14)\n",
    "    plt.ylabel('Estimated Volume (YOLO) (mL)', fontsize=14)\n",
    "    if hue_col or style_col: plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"scatter_est_vs_target_{filename_suffix}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if not analysis_df.empty: plot_scatter_estimated_vs_target(analysis_df, \"All Valid Data\", \"all_valid\")\n",
    "if not clean_df.empty: plot_scatter_estimated_vs_target(clean_df, \"Clean Data\", \"clean\")\n",
    "\n",
    "def plot_bland_altman(df_subset: pd.DataFrame, title_suffix: str, filename_suffix: str):\n",
    "    \"\"\"Creates a Bland-Altman plot.\"\"\"\n",
    "    df_plot_data = df_subset.dropna(subset=['target volume', 'estimated volume'])\n",
    "    if df_plot_data.empty or len(df_plot_data) < 2:\n",
    "        print(f\"No data or insufficient data for Bland-Altman plot for {title_suffix}\")\n",
    "        return\n",
    "\n",
    "    y_true = df_plot_data['target volume']\n",
    "    y_pred = df_plot_data['estimated volume']\n",
    "    \n",
    "    diff = y_pred - y_true\n",
    "    mean_val = (y_true + y_pred) / 2\n",
    "    \n",
    "    mean_diff = np.mean(diff)\n",
    "    std_diff = np.std(diff)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    hue_col = 'target place' if 'target place' in df_plot_data.columns else None\n",
    "    style_col = 'syringe_size_ml' if 'syringe_size_ml' in df_plot_data.columns else None\n",
    "    sns.scatterplot(x=mean_val, y=diff, data=df_plot_data, hue=hue_col, style=style_col, s=120, alpha=0.8)\n",
    "\n",
    "    plt.axhline(mean_diff, color='black', linestyle='-', label=f'Mean Diff: {mean_diff:.2f}', lw=2)\n",
    "    plt.axhline(mean_diff + 1.96 * std_diff, color='red', linestyle='--', label=f'+1.96 SD: {mean_diff + 1.96 * std_diff:.2f}', lw=1.5)\n",
    "    plt.axhline(mean_diff - 1.96 * std_diff, color='red', linestyle='--', label=f'-1.96 SD: {mean_diff - 1.96 * std_diff:.2f}', lw=1.5)\n",
    "    \n",
    "    plt.title(f'Bland-Altman Plot ({title_suffix})', fontsize=16)\n",
    "    plt.xlabel('Mean of Target and Estimated Volumes (mL)', fontsize=14)\n",
    "    plt.ylabel('Difference (Estimated - Target Volume) (mL)', fontsize=14)\n",
    "    if hue_col or style_col: plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"bland_altman_{filename_suffix}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if not analysis_df.empty: plot_bland_altman(analysis_df, \"All Valid Data\", \"all_valid\")\n",
    "if not clean_df.empty: plot_bland_altman(clean_df, \"Clean Data\", \"clean\")\n",
    "\n",
    "\n",
    "#  [markdown]\n",
    "# ## 6. Analysis by Factors\n",
    "#\n",
    "# Analyzing performance based on different experimental factors like target place, syringe type, and person.\n",
    "# We generally use `clean_df` for this factor analysis to reduce noise from known problematic readings.\n",
    "\n",
    "#\n",
    "if not clean_df.empty:\n",
    "    factor_analysis_df = clean_df.copy()\n",
    "elif not analysis_df.empty : # Fallback to analysis_df if clean_df is empty\n",
    "    print(\"Clean_df is empty, using analysis_df (all valid data) for factor analysis.\")\n",
    "    factor_analysis_df = analysis_df.copy()\n",
    "else:\n",
    "    factor_analysis_df = pd.DataFrame() # Empty df\n",
    "\n",
    "if factor_analysis_df.empty or 'target volume' not in factor_analysis_df.columns:\n",
    "    print(\"Skipping factor analysis as no suitable dataframe or 'target volume' column is available.\")\n",
    "else:\n",
    "    # Define volume ranges for analysis\n",
    "    # Ensure 'target volume' is suitable for pd.cut (numeric and not all NaN)\n",
    "    if pd.api.types.is_numeric_dtype(factor_analysis_df['target volume']) and factor_analysis_df['target volume'].notna().any():\n",
    "        bins = [0, 1, 3, 5, 10, 15, 20, np.inf]\n",
    "        labels = ['0-1mL', '1-3mL', '3-5mL', '5-10mL', '10-15mL', '15-20mL', '>20mL']\n",
    "        factor_analysis_df['volume_range_target'] = pd.cut(factor_analysis_df['target volume'], bins=bins, labels=labels, right=False)\n",
    "    else:\n",
    "        factor_analysis_df['volume_range_target'] = np.nan # Add column as NaN if cannot cut\n",
    "\n",
    "\n",
    "    factors = ['target place', 'syringe_size_ml', 'person', 'volume_range_target']\n",
    "    error_types_to_plot = ['error', 'abs_error', 'abs_percentage_error', 'error_normalized_by_syringe_capacity']\n",
    "    existing_factors = [f for f in factors if f in factor_analysis_df.columns]\n",
    "\n",
    "\n",
    "    for factor in existing_factors:\n",
    "        if factor_analysis_df[factor].nunique(dropna=False) < 1 : # Changed from <2 to <1 for cases where factor column itself might be all NaN\n",
    "            print(f\"\\nSkipping factor '{factor}' due to no unique values or all NaNs.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Metrics by {factor} (using {'Clean Data' if factor_analysis_df is clean_df else 'All Valid Data'}) ---\")\n",
    "        # Ensure groupby factor does not contain all NaNs if that's problematic for your pandas version with observed=False\n",
    "        if factor_analysis_df[factor].notna().any():\n",
    "            grouped = factor_analysis_df.groupby(factor, observed=False)\n",
    "\n",
    "            for name, group_df in grouped:\n",
    "                if len(group_df.dropna(subset=['target volume', 'estimated volume'])) > 1 :\n",
    "                    group_metrics = calculate_metrics(group_df['target volume'], group_df['estimated volume'], f\"{factor}: {name}\")\n",
    "                    if group_metrics: all_metrics_summary.append(group_metrics)\n",
    "                else:\n",
    "                    print(f\"Not enough data for {factor}: {name} (count: {len(group_df)})\")\n",
    "        else:\n",
    "            print(f\"Factor '{factor}' contains all NaN values, skipping grouping.\")\n",
    "\n",
    "\n",
    "        # Box plots for error distributions\n",
    "        if factor_analysis_df[factor].nunique(dropna=True) > 0:\n",
    "            num_err_types = len(error_types_to_plot)\n",
    "            valid_error_types_to_plot = [err for err in error_types_to_plot if err in factor_analysis_df.columns and factor_analysis_df[err].notna().any()]\n",
    "\n",
    "            if not valid_error_types_to_plot:\n",
    "                print(f\"No valid error types with data to plot for factor '{factor}'.\")\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, len(valid_error_types_to_plot), figsize=(6 * len(valid_error_types_to_plot), 6))\n",
    "            if len(valid_error_types_to_plot) == 1: axes = [axes]\n",
    "\n",
    "            for i, err_type in enumerate(valid_error_types_to_plot):\n",
    "                sns.boxplot(x=factor, y=err_type, data=factor_analysis_df, ax=axes[i], showfliers=True)\n",
    "                axes[i].set_title(f'{err_type.replace(\"_\", \" \").title()} by {factor.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "                axes[i].tick_params(axis='x', rotation=45 if factor_analysis_df[factor].nunique(dropna=True) > 4 else 0, labelsize=10)\n",
    "                axes[i].tick_params(axis='y', labelsize=10)\n",
    "                axes[i].set_xlabel(factor.replace(\"_\", \" \").title(), fontsize=12)\n",
    "                axes[i].set_ylabel(err_type.replace(\"_\", \" \").title(), fontsize=12)\n",
    "            plt.suptitle(f\"Error Distributions by {factor.replace('_', ' ').title()}\", fontsize=18, y=1.03)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            plt.savefig(f\"error_boxplot_by_{factor}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "    # FacetGrid for error distribution\n",
    "    if 'target place' in factor_analysis_df.columns and \\\n",
    "       factor_analysis_df['target place'].nunique(dropna=True) > 0 and \\\n",
    "       'error' in factor_analysis_df.columns and factor_analysis_df['error'].notna().any():\n",
    "        g = sns.FacetGrid(factor_analysis_df.dropna(subset=['error', 'target place']), col=\"target place\", col_wrap=3, sharex=False, sharey=False, height=5, aspect=1.2)\n",
    "        g.map(sns.histplot, \"error\", kde=True, bins=10)\n",
    "        g.set_titles(\"Error Distribution for {col_name}\", size=14)\n",
    "        g.set_axis_labels(\"Error (mL)\", \"Count\", fontsize=12)\n",
    "        plt.suptitle(\"Detailed Error Distribution by Target Place\", fontsize=18, y=1.03)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        plt.savefig(\"error_hist_by_target_place.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "#  [markdown]\n",
    "# ## 7. Impact of Noted Issues\n",
    "#\n",
    "# Compare performance for readings with and without specific issues noted (e.g., occlusion).\n",
    "# Uses 'analysis_df' to ensure all data with issues are captured.\n",
    "\n",
    "#\n",
    "if not analysis_df.empty and 'target volume' in analysis_df.columns and 'estimated volume' in analysis_df.columns:\n",
    "    # Impact of Occlusion\n",
    "    if 'is_occluded' in analysis_df.columns:\n",
    "        occluded_df = analysis_df[analysis_df['is_occluded'] == True]\n",
    "        not_occluded_df = analysis_df[analysis_df['is_occluded'] == False]\n",
    "\n",
    "        print(\"\\n--- Impact of Occlusion ---\")\n",
    "        occlusion_metrics = calculate_metrics(occluded_df['target volume'], occluded_df['estimated volume'], \"Occluded Readings\")\n",
    "        non_occlusion_metrics = calculate_metrics(not_occluded_df['target volume'], not_occluded_df['estimated volume'], \"Non-Occluded Readings (may include other issues)\")\n",
    "        \n",
    "        if occlusion_metrics: all_metrics_summary.append(occlusion_metrics)\n",
    "        if non_occlusion_metrics: all_metrics_summary.append(non_occlusion_metrics)\n",
    "    else:\n",
    "        print(\"Skipping occlusion impact analysis: 'is_occluded' column missing.\")\n",
    "\n",
    "    # Impact of 'Bad Readings'\n",
    "    if 'is_bad_reading' in analysis_df.columns:\n",
    "        bad_readings_df = analysis_df[analysis_df['is_bad_reading'] == True]\n",
    "        \n",
    "        print(\"\\n--- Impact of 'Bad Readings' (as noted) ---\")\n",
    "        bad_reading_metrics = calculate_metrics(bad_readings_df['target volume'], bad_readings_df['estimated volume'], \"Flagged Bad Readings\")\n",
    "        if bad_reading_metrics: all_metrics_summary.append(bad_reading_metrics)\n",
    "\n",
    "        if not clean_df.empty: # Compare with clean_df if available\n",
    "             clean_for_bad_comparison_metrics = calculate_metrics(clean_df['target volume'], clean_df['estimated volume'], \"Clean Readings (for Bad Reading comparison)\")\n",
    "             if clean_for_bad_comparison_metrics: all_metrics_summary.append(clean_for_bad_comparison_metrics)\n",
    "        else: # Fallback if clean_df is empty\n",
    "            good_readings_for_comparison_df = analysis_df[(analysis_df['is_bad_reading'] == False)]\n",
    "            good_reading_metrics_general = calculate_metrics(good_readings_for_comparison_df['target volume'], good_readings_for_comparison_df['estimated volume'], \"Not Flagged as Bad (may include other issues)\")\n",
    "            if good_reading_metrics_general: all_metrics_summary.append(good_reading_metrics_general)\n",
    "    else:\n",
    "        print(\"Skipping bad reading impact analysis: 'is_bad_reading' column missing.\")\n",
    "\n",
    "\n",
    "    # Display problematic readings details\n",
    "    print(\"\\n--- Details of All Problematic Readings (from analysis_df) ---\")\n",
    "    if 'problematic_reading' in analysis_df.columns and analysis_df['problematic_reading'].any():\n",
    "        # Ensure error columns are present\n",
    "        if 'error' not in analysis_df.columns:\n",
    "            analysis_df['error'] = analysis_df['estimated volume'] - analysis_df['target volume']\n",
    "        if 'abs_percentage_error' not in analysis_df.columns:\n",
    "             analysis_df['abs_percentage_error'] = np.abs((analysis_df['error'] / np.where(analysis_df['target volume'] == 0, np.finfo(float).eps, analysis_df['target volume'])) * 100)\n",
    "\n",
    "        problematic_cols_to_show = ['person', 'target volume', 'estimated volume', 'target place', 'syringe', 'notes',\n",
    "                                    'error', 'abs_percentage_error', 'is_occluded', 'is_bad_reading',\n",
    "                                    'is_manual_extraction', 'other_issues']\n",
    "        existing_problematic_cols = [col for col in problematic_cols_to_show if col in analysis_df.columns]\n",
    "        problematic_details_df = analysis_df[analysis_df['problematic_reading']]\n",
    "\n",
    "        if not problematic_details_df.empty and existing_problematic_cols:\n",
    "            print(problematic_details_df[existing_problematic_cols].to_string())\n",
    "        else:\n",
    "            print(\"No problematic readings found or relevant columns missing for details.\")\n",
    "    else:\n",
    "        print(\"No problematic readings found based on current flags in analysis_df or 'problematic_reading' column missing.\")\n",
    "\n",
    "#  [markdown]\n",
    "# ## 8. Statistical Significance (Optional but Recommended)\n",
    "\n",
    "#\n",
    "if not clean_df.empty and 'error' in clean_df.columns and clean_df['error'].notna().any() and \\\n",
    "   'target volume' in clean_df.columns and 'estimated volume' in clean_df.columns:\n",
    "    diff_clean = clean_df['error'].dropna()\n",
    "    if len(diff_clean) > 7: # Shapiro needs more than a few samples\n",
    "        shapiro_test_stat, shapiro_p_value = stats.shapiro(diff_clean)\n",
    "        print(\"\\n--- Normality Test for Differences (Clean Data Error) ---\")\n",
    "        print(f\"Shapiro-Wilk Test Statistic: {shapiro_test_stat:.3f}, P-value: {shapiro_p_value:.3f}\")\n",
    "\n",
    "        test_to_use = \"\"\n",
    "        if shapiro_p_value > 0.05:\n",
    "            print(\"Differences appear normally distributed. Using Paired t-test.\")\n",
    "            test_to_use = \"ttest\"\n",
    "        else:\n",
    "            print(\"Differences do not appear normally distributed. Using Wilcoxon signed-rank test.\")\n",
    "            test_to_use = \"wilcoxon\"\n",
    "        \n",
    "        target_clean = clean_df['target volume'].dropna()\n",
    "        estimated_clean = clean_df['estimated volume'].loc[target_clean.index].dropna()\n",
    "        target_clean = target_clean.loc[estimated_clean.index]\n",
    "\n",
    "        if len(target_clean) > 1:\n",
    "            if test_to_use == \"ttest\":\n",
    "                ttest_result = stats.ttest_rel(target_clean, estimated_clean, nan_policy='omit')\n",
    "                print(\"\\n--- Paired t-test (Target vs. Estimated on Clean Data) ---\")\n",
    "                print(f\"T-statistic: {ttest_result.statistic:.3f}\")\n",
    "                print(f\"P-value: {ttest_result.pvalue:.3f}\")\n",
    "                if ttest_result.pvalue < 0.05:\n",
    "                    print(\"The difference between target and estimated volumes is statistically significant (p < 0.05).\")\n",
    "                else:\n",
    "                    print(\"No statistically significant difference found between target and estimated volumes (p >= 0.05).\")\n",
    "            elif test_to_use == \"wilcoxon\":\n",
    "                # Wilcoxon needs differences, which is `diff_clean`\n",
    "                # Ensure `diff_clean` corresponds to paired observations, which it should if `error` was calculated correctly\n",
    "                if len(diff_clean) > 0 : # Wilcoxon might need non-zero differences for some versions or cases\n",
    "                    try:\n",
    "                        wilcoxon_result = stats.wilcoxon(diff_clean, nan_policy='omit')\n",
    "                        print(\"\\n--- Wilcoxon Signed-Rank Test (on Clean Data Error) ---\")\n",
    "                        print(f\"Statistic: {wilcoxon_result.statistic:.3f}\")\n",
    "                        print(f\"P-value: {wilcoxon_result.pvalue:.3f}\")\n",
    "                        if wilcoxon_result.pvalue < 0.05:\n",
    "                            print(\"The median difference between target and estimated volumes is statistically significant (p < 0.05).\")\n",
    "                        else:\n",
    "                            print(\"No statistically significant median difference found (p >= 0.05).\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Could not perform Wilcoxon test: {e}. This can happen if all differences are zero.\")\n",
    "                else:\n",
    "                    print(\"Not enough non-zero differences for Wilcoxon test.\")\n",
    "        else:\n",
    "            print(\"Not enough aligned clean data for paired statistical test after NaN handling.\")\n",
    "    else:\n",
    "        print(\"\\nNot enough clean data (or errors) for normality test or subsequent paired tests.\")\n",
    "else:\n",
    "    print(\"\\nSkipping paired statistical tests: Clean data, error column, or target/estimated columns are missing or empty.\")\n",
    "\n",
    "\n",
    "# ANOVA example for comparing mean errors across 'target place' using clean data\n",
    "if not clean_df.empty and 'error' in clean_df.columns and clean_df['error'].notna().any() and \\\n",
    "   'target place' in clean_df.columns and clean_df['target place'].nunique(dropna=True) > 1:\n",
    "    \n",
    "    model_formula = 'error ~ C(target_place)'\n",
    "    try:\n",
    "        anova_data = clean_df[['error', 'target_place']].dropna()\n",
    "        if anova_data['target_place'].nunique() > 1 and len(anova_data) > anova_data['target_place'].nunique() * 2 :\n",
    "            model = ols(model_formula, data=anova_data).fit()\n",
    "            anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "            print(\"\\n--- ANOVA for Error by Target Place (Clean Data) ---\")\n",
    "            print(anova_table)\n",
    "        else:\n",
    "            print(\"\\nNot enough data or unique groups for ANOVA on 'error' by 'target_place'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not perform ANOVA for 'error' by 'target_place': {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping ANOVA: Clean data, 'error', or 'target_place' column issues, or not enough groups.\")\n",
    "\n",
    "\n",
    "#  [markdown]\n",
    "# ## 9. Summary of Metrics and Conclusion\n",
    "\n",
    "#\n",
    "print(\"\\n--- Summary of All Calculated Metrics ---\")\n",
    "if all_metrics_summary:\n",
    "    summary_df = pd.DataFrame(all_metrics_summary)\n",
    "    cols_order = ['Dataset', 'Count', 'MAE', 'RMSE', 'MAPE (%)', 'Mean Error (Bias)', 'R2', 'Pearson Correlation', \"Lin's CCC\", 'MSE']\n",
    "    # Ensure only existing columns are selected for reordering\n",
    "    existing_cols_for_summary = [col for col in cols_order if col in summary_df.columns]\n",
    "    summary_df = summary_df[existing_cols_for_summary]\n",
    "    print(summary_df.to_string())\n",
    "    try:\n",
    "        summary_df.to_csv(\"metrics_summary_yolo_analysis.csv\", index=False)\n",
    "        print(\"\\nMetrics summary saved to 'metrics_summary_yolo_analysis.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving metrics summary to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No metrics were calculated to summarize.\")\n",
    "\n",
    "#  [markdown]\n",
    "# ### Discussion Points:\n",
    "# * **Overall Performance:** Review MAE, RMSE, R2, Bias from \"All Valid Data\" and \"Clean Data\".\n",
    "# * **Impact of Factors:**\n",
    "#     * `target place`, `syringe size`, `person`, `volume_range_target`: How do errors (MAE, MAPE, bias) vary? Are these variations statistically significant (ANOVA results)?\n",
    "#     * `error_normalized_by_syringe_capacity`: Does this reveal different insights than raw error?\n",
    "# * **Problematic Readings:**\n",
    "#     * Quantify the impact of occlusions, \"bad readings\", etc., on metrics. Refer to \"Impact of Noted Issues\" section.\n",
    "#     * Notes like \"syringe on table disappears\", \"manual extraction\", \"person8 in folder\" are critical. Discuss their implications and how they were handled (e.g., exclusion via `problematic_reading` flag).\n",
    "# * **Comparison with Volume from Weight:** How does `actual_liquid_volume_from_weight` compare to `target_volume` (from EDA)? Discrepancies here indicate issues with the ground truth preparation itself.\n",
    "# * **Statistical Significance:**\n",
    "#     * Was there a systematic bias in the YOLO estimates (paired t-test/Wilcoxon)?\n",
    "#     * Did error significantly differ across factor levels (ANOVA/Kruskal-Wallis)?\n",
    "# * **Data Quality & Model Limitations:**\n",
    "#     * Address `NaN` estimated volumes: what types of scenarios (notes, conditions) led to model failure?\n",
    "#     * \"no syringe detected,\" \"corrupted video\": these are data loss scenarios.\n",
    "# * **Limitations of this Analysis:** Sample size, specific conditions, assumptions of statistical tests.\n",
    "# * **Future Work:** Model retraining with more diverse/problematic data, specific occlusion handling, improvements to experimental protocol based on findings.\n",
    "\n",
    "#  [markdown]\n",
    "# --- End of Notebook ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo11-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
